#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Copyright 2025 QozeCode

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""

import argparse
import asyncio
import operator
# å±è”½ absl åº“çš„ STDERR è­¦å‘Š
# import os
import traceback
import uuid
from typing import Literal
import platform
import os
import socket
# import nest_asyncio
# from langchain_community.agent_toolkits import PlayWrightBrowserToolkit
# from langchain_community.tools.playwright.utils import create_async_playwright_browser
from langchain_core.messages import AnyMessage, AIMessage
from langchain_core.messages import HumanMessage
from langchain_core.messages import SystemMessage
from langchain_core.messages import ToolMessage
from langgraph.graph import StateGraph, START, END
from rich.panel import Panel
from typing_extensions import TypedDict, Annotated

from completion_handler import setup_completion
from input_handler import input_manager
from input_processor import InputProcessor
from shared_console import console
from stream_output import StreamOutput
# from tools.common_tools import ask
from tools.execute_command_tool import execute_command
from tools.math_tools import multiply, add, divide
from tools.search_tool import tavily_search, get_webpage_to_markdown
from utils.directory_tree import get_directory_tree
from utils.system_prompt import get_system_prompt

os.environ.setdefault('ABSL_LOGGING_VERBOSITY', '1')  # åªæ˜¾ç¤º WARNING åŠä»¥ä¸Šçº§åˆ«
os.environ.setdefault('TF_CPP_MIN_LOG_LEVEL', '2')  # å±è”½ TensorFlow ä¿¡æ¯å’Œè­¦å‘Š

# å®šä¹‰é¢œè‰²å¸¸é‡
CYAN = "\033[96m"
RESET = "\033[0m"

# å…¨å±€ LLM å˜é‡ï¼Œå°†åœ¨ main å‡½æ•°ä¸­åˆå§‹åŒ–
llm = None
llm_with_tools = None
browser_tools = None

base_tools = [add, multiply, divide, execute_command, tavily_search, get_webpage_to_markdown]

# åˆå§‹æ—¶ä¸åŠ è½½æµè§ˆå™¨å·¥å…·
tools = base_tools
browser_loaded = False
plan_mode = False
# å½“å‰ä¼šè¯
conversation_state = {"messages": [], "llm_calls": 0}


def get_terminal_display_lines():
    """è·å–ç»ˆç«¯å¯ç”¨äºæ˜¾ç¤ºå†…å®¹çš„è¡Œæ•°"""
    try:
        terminal_height = console.size.height
        return max(10, terminal_height - 8)
    except:
        # å¦‚æœè·å–ç»ˆç«¯å¤§å°å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
        return 20


tools_by_name = {tool.name: tool for tool in tools}


# Step 1: Define state

class MessagesState(TypedDict):
    messages: Annotated[list[AnyMessage], operator.add]
    llm_calls: int


# Step 2: Define model node
def llm_call(state: dict):
    # messages = state["messages"]
    system_release = ''
    system_info = platform.system()
    system_version = "unknown"
    current_dir = os.getcwd()
    username = os.getenv('USER', 'unknown')
    hostname = socket.gethostname()

    shell = home_dir = "unknown"
    machine_type = processor = "unknown"
    directory_tree = "æ— æ³•è·å–ç›®å½•ç»“æ„"
    # è·å–ç³»ç»Ÿä¿¡æ¯
    try:
        # åŸºæœ¬ç³»ç»Ÿä¿¡æ¯
        system_info = platform.system()
        system_version = platform.version()
        system_release = platform.release()
        machine_type = platform.machine()
        processor = platform.processor()
        # å½“å‰å·¥ä½œç›®å½•
        current_dir = os.getcwd()
        # ç”¨æˆ·ä¿¡æ¯
        username = os.getenv('USER') or os.getenv('USERNAME') or 'unknown'
        # ä¸»æœºå
        hostname = socket.gethostname()
        # ç¯å¢ƒå˜é‡ä¸­çš„é‡è¦ä¿¡æ¯
        shell = os.getenv('SHELL', 'unknown')
        home_dir = os.getenv('HOME', 'unknown')
        directory_tree = get_directory_tree(current_dir)

    except Exception:
        print("è·å–è®¾å¤‡ä¿¡æ¯å¼‚å¸¸")

    system_msg = get_system_prompt(system_info=system_info, system_release=system_release,
                                   system_version=system_version, machine_type=machine_type, processor=processor,
                                   hostname=hostname, username=username, shell=shell, current_dir=current_dir,
                                   home_dir=home_dir, directory_tree=directory_tree, plan_mode=plan_mode)

    # # è¿‡æ»¤æ‰ä¹‹å‰çš„ SystemMessageï¼Œåªä¿ç•™æœ€æ–°çš„ï¼Œå¹¶æ¸…ç†æ–‡æœ¬
    # non_system_messages = []
    # for msg in messages:
    #     if not isinstance(msg, SystemMessage):
    #         non_system_messages.append(msg)
    #
    # final_messages = [system_msg] + non_system_messages
    # print(f"è¯·æ±‚AIä¸Šä¸‹æ–‡:{final_messages}")

    # converted_messages = []
    # for msg in state["messages"]:
    #     if isinstance(msg, AIMessage):
    #         # æ ¹æ®æ‚¨çš„éœ€æ±‚æ„å»º JSON å¯¹è±¡
    #         json_msg = msg.model_dump()
    #         json_msg['reasoning_content'] = msg.additional_kwargs['reasoning_content']
    #         print(f"AIMessage={json_msg}")
    #         converted_messages.append(json_msg)
    #     else:
    #         converted_messages.append(msg)  # å…¶ä»–ç±»å‹æ¶ˆæ¯ä¿æŒä¸å˜
    #
    # # ç„¶åä½¿ç”¨ converted_messages
    #
    # return {
    #     "messages": [
    #         llm_with_tools.invoke([SystemMessage(content=system_msg)] + converted_messages)
    #     ],
    #     "llm_calls": state.get('llm_calls', 0) + 1
    # }

    # print(f"messages={[
    #                       SystemMessage(
    #                           content=system_msg
    #                       )
    #                   ]
    #                   + state["messages"]}")
    # print(f"llm_with_tools={llm_with_tools}")
    return {
        "messages": [
            llm_with_tools.invoke(
                [
                    SystemMessage(
                        content=system_msg
                    )
                ]
                + state["messages"]
            )
        ],
        "llm_calls": state.get('llm_calls', 0) + 1
    }


# Step 3: Define tool node
async def tool_node(state: dict):
    """Performs the tool call"""

    result = []
    for tool_call in state["messages"][-1].tool_calls:
        tool = tools_by_name[tool_call["name"]]
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¼‚æ­¥å·¥å…·
            if tool_call["name"] in ["tavily_search", "get_webpage_to_markdown"]:
                observation = await tool.ainvoke(tool_call["args"])
            else:
                observation = tool.invoke(tool_call["args"])
            result.append(ToolMessage(content=observation, tool_call_id=tool_call["id"]))
        except Exception as e:
            traceback.print_exc()
            error_msg = f"  âŒ '{tool_call['name']}' è°ƒç”¨å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯:{e}"
            console.print(error_msg, style="red")
            result.append(ToolMessage(content=error_msg, tool_call_id=tool_call["id"]))
    return {"messages": result}


# Step 4: Define logic to determine whether to end
def should_continue(state: MessagesState) -> Literal["tool_node", END]:
    """Decide if we should continue the loop or stop based upon whether the LLM made a tool call"""

    messages = state["messages"]
    last_message = messages[-1]
    # If the LLM makes a tool call, then perform an action
    if last_message.tool_calls:
        return "tool_node"
    return END


# Step 5: Build agent
# Build workflow
agent_builder = StateGraph(MessagesState)

# Add nodes
agent_builder.add_node("llm_call", llm_call)
agent_builder.add_node("tool_node", tool_node)

# Add edges to connect nodes
agent_builder.add_edge(START, "llm_call")
agent_builder.add_conditional_edges(
    "llm_call",
    should_continue,
    ["tool_node", END]
)
agent_builder.add_edge("tool_node", "llm_call")

# Compile the agent
agent = agent_builder.compile()


# å¤šè½®å¯¹è¯å‡½æ•°
async def chat_loop(session_id: str = None, model_name: str = None):
    global plan_mode
    os.system('cls' if os.name == 'nt' else 'clear')
    combined_panel = Panel(
        f"[bold dim cyan]âœ¦ Welcome to QozeCode 0.2.3[/bold dim cyan]\n\n"
        f"[bold white]æ¨¡å‹:[/bold white][bold cyan] {model_name or 'Unknown'}[bold cyan]\n"
        f"[bold white]ä½¿ç”¨æç¤º:[/bold white]\n"
        f"[dim][bold white]  â€¢ è¾“å…¥ [bold]'q'[/bold]ã€[bold]'quit'[/bold] æˆ– [bold]'exit'[/bold] é€€å‡º [/dim] [bold white]\n"
        f"[dim][bold white]  â€¢ !å¼€å¤´ä¼šç›´æ¥æ‰§è¡Œä¾‹å¦‚ï¼š!ls [/dim] [bold white]",
        border_style="dim white",
        title_align="center",
        expand=False
    )
    console.print(combined_panel)

    # åˆå§‹åŒ–å¤„ç†å™¨
    input_processor = InputProcessor(input_manager)
    stream_output = StreamOutput(agent)

    while True:
        try:
            # è®¾ç½®è‡ªåŠ¨è¡¥å…¨
            setup_completion()
            # è¾“å…¥å¤„ç†
            user_input = await input_processor.get_user_input(session_id, plan_mode)

            if user_input.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                console.print("ğŸ‘‹ å†è§ï¼", style="bold cyan")
                return

            if user_input.lower() in ['plan']:
                plan_mode = True
                console.print("è¿›å…¥è®¡åˆ’æ¨¡å¼")
                continue

            # ç©ºè¾“å…¥ï¼Œç»§ç»­å¾ªç¯
            if user_input == "":
                continue

            # åˆ›å»ºç”¨æˆ·æ¶ˆæ¯
            user_message = HumanMessage(content=user_input)
            # æ›´æ–°å¯¹è¯çŠ¶æ€
            current_state = {
                "messages": conversation_state["messages"] + [user_message],
                "llm_calls": conversation_state["llm_calls"]
            }
            # æµå¼è¾“å‡º
            await stream_output.stream_response(model_name, current_state, conversation_state)

        except KeyboardInterrupt:
            console.print("\n\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­", style="yellow")
            break

        except Exception as e:
            traceback.print_exc()
            console.print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}", style="red")


async def start_chat_with_session(session_id: str = None, model_name: str = None):
    """å¯åŠ¨å¸¦ä¼šè¯ ID çš„èŠå¤©"""
    await chat_loop(session_id, model_name)


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='QozeCode Agent - AIç¼–ç¨‹åŠ©æ‰‹')
    parser.add_argument(
        '--model',
        choices=['claude-4', 'gemini', 'gpt-5'],
        default='gemini',
        help='é€‰æ‹©è¦ä½¿ç”¨çš„AIæ¨¡å‹ (é»˜è®¤: gemini)'
    )
    parser.add_argument(
        '--session-id',
        default='123',
        help='ä¼šè¯ID (é»˜è®¤: 123)'
    )
    return parser.parse_args()


def handleRun(model_name: str = None, session_id: str = None):
    """ä¸»å‡½æ•° - æ”¯æŒç›´æ¥ä¼ å…¥å‚æ•°æˆ–ä»å‘½ä»¤è¡Œè§£æ"""
    try:
        # åˆå§‹åŒ–é€‰æ‹©çš„æ¨¡å‹ï¼ˆä»…æ„å»ºå®¢æˆ·ç«¯ï¼Œä¸åšç½‘ç»œéªŒè¯ï¼‰
        with console.status("[bold cyan]æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...", spinner="dots"):
            # å»¶è¿Ÿå¯¼å…¥ä»¥é¿å…å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹ç›¸å…³é‡ä¾èµ–
            from model_initializer import initialize_llm
            global llm, llm_with_tools
            llm = initialize_llm(model_name)
            # åˆå§‹åŒ–å¸¦å·¥å…·çš„ LLM
            llm_with_tools = llm.bind_tools(tools)
        # å¯åŠ¨èŠå¤©å¾ªç¯
        asyncio.run(start_chat_with_session(session_id, model_name))

    except KeyboardInterrupt:
        console.print("\n\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­", style="yellow")
    except Exception as e:
        console.print(f"\nâŒ å¯åŠ¨å¤±è´¥: {str(e)}", style="red")
