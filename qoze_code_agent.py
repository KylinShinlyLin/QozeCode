from langgraph.checkpoint.memory import MemorySaver

# !/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Copyright 2025 QozeCode

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""

import base64
import operator
import platform
import traceback
from typing import Literal, List
from langchain_core.messages import AnyMessage
from langchain_core.messages import HumanMessage
from langchain_core.messages import SystemMessage
from langchain_core.messages import ToolMessage
from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict, Annotated
from shared_console import console
import sys
import os

from tools.execute_command_tool import execute_command

sys.path.append(os.path.join(os.path.dirname(__file__), '.qoze'))
from tools.search_tool import tavily_search, get_webpage_to_markdown
from tools.skill_tools import activate_skill, list_available_skills, deactivate_skill
from skills.skill_manager import SkillManager
from utils.directory_tree import get_directory_tree
from utils.system_prompt import get_system_prompt

os.environ.setdefault('ABSL_LOGGING_VERBOSITY', '1')  # åªæ˜¾ç¤º WARNING åŠä»¥ä¸Šçº§åˆ«
os.environ.setdefault('TF_CPP_MIN_LOG_LEVEL', '2')  # å±è”½ TensorFlow ä¿¡æ¯å’Œè­¦å‘Š

# å®šä¹‰é¢œè‰²å¸¸é‡
CYAN = "\033[96m"
RESET = "\033[0m"

# å…¨å±€æŠ€èƒ½ç®¡ç†å™¨
skill_manager = None


def get_enhanced_system_prompt(system_info="", system_release="", system_version="", machine_type="", processor="",
                               shell="", current_dir="", directory_tree=""):
    """è·å–å¢å¼ºçš„ç³»ç»Ÿæç¤ºè¯ï¼ˆåŒ…å«æŠ€èƒ½ä¿¡æ¯ï¼‰"""
    # å…¼å®¹ ~/.qoze/skills ä¸‹çš„è„šæœ¬è°ƒç”¨
    user_qoze_path = os.path.expanduser("~/.qoze")
    if user_qoze_path not in sys.path:
        sys.path.append(user_qoze_path)

    global skill_manager
    if skill_manager is None:
        skill_manager = SkillManager()
    base_prompt = get_system_prompt(system_info=system_info, system_release=system_release,
                                    system_version=system_version, machine_type=machine_type, processor=processor,
                                    shell=shell, current_dir=current_dir, directory_tree=directory_tree)
    available_skills = skill_manager.get_available_skills()
    active_skills_content = skill_manager.get_active_skills_content()
    skills_prompt = ""
    if available_skills:
        skills_list = [f"- **{name}**: {description}" for name, description in available_skills.items()]
        skills_prompt = "\n\n## ğŸ¯ Available Skills System\n" + "\n".join(skills_list)
    if active_skills_content:
        skills_prompt += f"\n\n## ğŸ”¥ Currently Active Skills:\n{active_skills_content}"
    return base_prompt + skills_prompt


# å…¨å±€ LLM å˜é‡ï¼Œå°†åœ¨ main å‡½æ•°ä¸­åˆå§‹åŒ–
llm = None
llm_with_tools = None
browser_tools = None

base_tools = [execute_command, tavily_search, get_webpage_to_markdown, activate_skill, list_available_skills,
              deactivate_skill]

# åˆå§‹æ—¶ä¸åŠ è½½æµè§ˆå™¨å·¥å…·
tools = base_tools
browser_loaded = False
plan_mode = False
# å½“å‰ä¼šè¯
conversation_state = {"messages": [], "llm_calls": 0}

tools_by_name = {tool.name: tool for tool in tools}


# Step 1: Define state
class MessagesState(TypedDict):
    messages: Annotated[list[AnyMessage], operator.add]
    llm_calls: int


# Step 2: Define model node
async def llm_call(state: dict):
    import asyncio
    system_release = ''
    system_info = platform.system()
    system_version = "unknown"
    current_dir = os.getcwd()

    shell = "unknown"
    machine_type = processor = "unknown"
    directory_tree = "æ— æ³•è·å–ç›®å½•ç»“æ„"
    try:
        system_info = platform.system()
        system_version = platform.version()
        system_release = platform.release()
        machine_type = platform.machine()
        processor = platform.processor()
        current_dir = os.getcwd()
        shell = os.getenv('SHELL', 'unknown')
        # Run directory tree generation in a thread to avoid blocking
        directory_tree = await asyncio.to_thread(get_directory_tree, current_dir)

    except Exception:
        print("è·å–è®¾å¤‡ä¿¡æ¯å¼‚å¸¸")

    system_msg = get_enhanced_system_prompt(system_info=system_info, system_release=system_release,
                                            system_version=system_version, machine_type=machine_type,
                                            processor=processor, shell=shell, current_dir=current_dir,
                                            directory_tree=directory_tree)

    # Use ainvoke for non-blocking LLM call
    response = await llm_with_tools.ainvoke(
        [SystemMessage(content=system_msg)] + state["messages"]
    )

    return {
        "messages": [response],
        "llm_calls": state.get('llm_calls', 0) + 1
    }


# Step 3: Define tool node
async def tool_node(state: dict):
    """Performs the tool call"""

    result = []
    for tool_call in state["messages"][-1].tool_calls:
        tool = tools_by_name[tool_call["name"]]
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¼‚æ­¥å·¥å…·
            if tool_call["name"] in ["tavily_search", "get_webpage_to_markdown", "execute_command"]:
                observation = await tool.ainvoke(tool_call["args"])
            else:
                observation = tool.invoke(tool_call["args"])
            result.append(ToolMessage(content=observation, tool_call_id=tool_call["id"]))
        except Exception as e:
            traceback.print_exc()
            error_msg = f"  âŒ '{tool_call['name']}' è°ƒç”¨å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯:{e}"
            console.print(error_msg, style="red")
            result.append(ToolMessage(content=error_msg, tool_call_id=tool_call["id"]))
    return {"messages": result}


# Step 4: Define logic to determine whether to end
def should_continue(state: MessagesState) -> Literal["tool_node", END]:
    """Decide if we should continue the loop or stop based upon whether the LLM made a tool call"""

    messages = state["messages"]
    last_message = messages[-1]
    # If the LLM makes a tool call, then perform an action
    if last_message.tool_calls:
        return "tool_node"
    return END


# Step 5: Build agent
# Build workflow
agent_builder = StateGraph(MessagesState)

# Add nodes
agent_builder.add_node("llm_call", llm_call)
agent_builder.add_node("tool_node", tool_node)

# Add edges to connect nodes
agent_builder.add_edge(START, "llm_call")
agent_builder.add_conditional_edges(
    "llm_call",
    should_continue,
    ["tool_node", END]
)
agent_builder.add_edge("tool_node", "llm_call")

# Compile the agent
memory = MemorySaver()
agent = agent_builder.compile(checkpointer=memory)


def get_image_files(folder_path: str) -> List[str]:
    """è·å–æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶"""
    if not os.path.exists(folder_path):
        return []

    image_extensions = {'.jpg', '.jpeg', '.png', '.webp'}
    image_files = []

    try:
        for file in os.listdir(folder_path):
            file_path = os.path.join(folder_path, file)
            if os.path.isfile(file_path):
                _, ext = os.path.splitext(file.lower())
                if ext in image_extensions:
                    image_files.append(file_path)
    except Exception as e:
        console.print(f"è¯»å–å›¾ç‰‡æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}", style="red")

    return image_files


def image_to_base64(image_path: str) -> str:
    """å°†å›¾ç‰‡æ–‡ä»¶è½¬æ¢ä¸ºbase64ç¼–ç """
    try:
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')
    except Exception as e:
        console.print(f"è½¬æ¢å›¾ç‰‡ {image_path} ä¸ºbase64æ—¶å‡ºé”™: {str(e)}", style="yellow")
        return None


def create_message_with_images(text_content: str, image_folder: str = ".qoze/image") -> HumanMessage:
    """åˆ›å»ºåŒ…å«æ–‡æœ¬å’Œå›¾ç‰‡çš„æ¶ˆæ¯"""
    # åŸºç¡€æ¶ˆæ¯å†…å®¹
    message_content = [{"type": "text", "text": text_content}]

    # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶å¤¹
    if os.path.exists(image_folder):
        image_files = get_image_files(image_folder)

        if image_files:
            # æ·»åŠ å›¾ç‰‡åˆ°æ¶ˆæ¯å†…å®¹
            for image_path in image_files[:5]:  # é™åˆ¶æœ€å¤š5å¼ å›¾ç‰‡ï¼Œé¿å…è¯·æ±‚è¿‡å¤§
                base64_data = image_to_base64(image_path)
                if base64_data:
                    # è·å–å›¾ç‰‡æ–‡ä»¶åç”¨äºæ ‡è¯†
                    image_name = os.path.basename(image_path)
                    # æ·»åŠ å›¾ç‰‡æ•°æ®
                    message_content.append({
                        "mime_type": "image/jpeg",
                        "type": "image",
                        "source_type": "base64",
                        "data": base64_data
                    })

            if len(image_files) > 5:
                console.print(f"âš ï¸  å›¾ç‰‡æ•°é‡è¶…è¿‡5å¼ ï¼Œåªå‘é€å‰5å¼ ", style="yellow")

    return HumanMessage(content=message_content)
